{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import collocations\n",
    "from nltk.collocations import *\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "n-gram - Wikipedia, the free encyclopedia\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "a:lang(ar),a:lang(kk-arab),a:lang(mzn),a:lang(ps),a:lang(ur){text-decoration:none}\n",
      "/* cache key: global:resourceloader:filter:minify-css:7:de1ab5287c9076b96eedd3f97a84a7b6 */\n",
      "\n",
      "if(window.mw){\n",
      "mw.config.set({\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"N-gram\",\"wgTitle\":\"N-gram\",\"wgCurRevisionId\":669294742,\"wgRevisionId\":669294742,\"wgArticleId\":986182,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"CS1 maint: Explicit use of et al.\",\"Articles to be merged from May 2015\",\"All articles to be merged\",\"Articles lacking in-text citations from February 2011\",\"All articles lacking in-text citations\",\"All articles with specifically marked weasel-worded phrases\",\"Articles with specifically marked weasel-worded phrases from November 2011\",\"Articles with specifically marked weasel-worded phrases from June 2014\",\"All articles with unsourced statements\",\"Articles with unsourced statements from November 2011\",\"Natural language processing\",\"Computational linguistics\",\"Language modeling\",\"Speech recognition\",\"Corpus linguistics\",\"Probabilistic models\"],\"wgBreakFrames\":false,\"wgPageContentLanguage\":\"en\",\"wgPageContentModel\":\"wikitext\",\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgMonthNamesShort\":[\"\",\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"],\"wgRelevantPageName\":\"N-gram\",\"wgRelevantArticleId\":986182,\"wgIsProbablyEditable\":true,\"wgRestrictionEdit\":[],\"wgRestrictionMove\":[],\"wgMediaViewerOnClick\":true,\"wgMediaViewerEnabledByDefault\":true,\"wgVisualEditor\":{\"pageLanguageCode\":\"en\",\"pageLanguageDir\":\"ltr\",\"usePageImages\":true,\"usePageDescriptions\":true},\"wikilove-recipient\":\"\",\"wikilove-anon\":0,\"wgPoweredByHHVM\":true,\"wgULSAcceptLanguageList\":[\"en-us\",\"en\"],\"wgULSCurrentAutonym\":\"English\",\"wgWikiEditorEnabledModules\":{\"toolbar\":true,\"dialogs\":true,\"hidesig\":true,\"preview\":false,\"publish\":false},\"wgBetaFeaturesFeatures\":[],\"wgGatherShouldShowTutorial\":true,\"wgFlaggedRevsParams\":{\"tags\":{\"status\":{\"levels\":1,\"quality\":2,\"pristine\":3}}},\"wgStableRevisionId\":null,\"wgCategoryTreePageCategoryOptions\":\"{\\\"mode\\\":0,\\\"hideprefix\\\":20,\\\"showcount\\\":true,\\\"namespaces\\\":false}\",\"wgNoticeProject\":\"wikipedia\",\"wgWikibaseItemId\":\"Q94489\"});\n",
      "}if(window.mw){\n",
      "mw.loader.implement(\"user.options\",function($,jQuery){mw.user.options.set({\"variant\":\"en\"});});\n",
      "/* cache key: global:resourceloader:filter:minify-js:7:b2706269305541eba923c165462b22c4 */\n",
      "}\n",
      "if(window.mw){\n",
      "mw.loader.implement(\"user.tokens\",function($,jQuery){mw.user.tokens.set({\"editToken\":\"+\\\\\",\"patrolToken\":\"+\\\\\",\"watchToken\":\"+\\\\\"});});\n",
      "}\n",
      "if(window.mw){\n",
      "mw.loader.load([\"mediawiki.page.startup\",\"mediawiki.legacy.wikibits\",\"mediawiki.legacy.ajax\",\"ext.centralauth.centralautologin\",\"mmv.head\",\"ext.imageMetrics.head\",\"ext.visualEditor.viewPageTarget.init\",\"ext.uls.init\",\"ext.uls.interface\",\"ext.centralNotice.bannerController\",\"skins.vector.js\"]);\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "n-gram\n",
      "\n",
      "From Wikipedia, the free encyclopedia\n",
      "\n",
      "\n",
      "\t\t\t\t\tJump to:\t\t\t\t\tnavigation, \t\t\t\t\tsearch\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "It has been suggested that this article be merged with K-mer. (Discuss) Proposed since May 2015.\n",
      "\n",
      "\n",
      "Not to be confused with engram.\n",
      "For Google phrase-usage graphs, see Google Ngram Viewer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "This article includes a list of references, but its sources remain unclear because it has insufficient inline citations. Please help to improve this article by introducing more precise citations. (February 2011)\n",
      "\n",
      "\n",
      "\n",
      "In the fields of computational linguistics and probability, an n-gram is a contiguous sequence of n items from a given sequence of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application. The n-grams typically are collected from a text or speech corpus. When the items are words, n-grams may also be called shingles.[1]\n",
      "An n-gram of size 1 is referred to as a \"unigram\"; size 2 is a \"bigram\" (or, less commonly, a \"digram\"); size 3 is a \"trigram\". Larger sizes are sometimes referred to by the value of n, e.g., \"four-gram\", \"five-gram\", and so on.\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "\n",
      "1 Applications\n",
      "2 Examples\n",
      "3 n-gram models\n",
      "4 Applications and considerations\n",
      "5 n-grams for approximate matching\n",
      "6 Other applications\n",
      "7 Bias-versus-variance trade-off\n",
      "\n",
      "7.1 Smoothing techniques\n",
      "7.2 Skip-gram\n",
      "\n",
      "\n",
      "8 Syntactic n-grams\n",
      "9 See also\n",
      "10 References\n",
      "11 External links\n",
      "\n",
      "\n",
      "\n",
      "Applications[edit]\n",
      "An n-gram model is a type of probabilistic language model for predicting the next item in such a sequence in the form of a (n − 1)–order Markov model.[2] n-gram models are now widely used in probability, communication theory, computational linguistics (for instance, statistical natural language processing), computational biology (for instance, biological sequence analysis), and data compression. The two core advantages[compared to?] of n-gram models (and algorithms that use them) are relative simplicity and the ability to scale up – by simply increasing n a model can be used to store more context with a well-understood space–time tradeoff, enabling small experiments to scale up very efficiently.\n",
      "Examples[edit]\n",
      "\n",
      "Figure 1 n-gram examples from various disciplines\n",
      "\n",
      "Field\n",
      "Unit\n",
      "Sample sequence\n",
      "1-gram sequence\n",
      "2-gram sequence\n",
      "3-gram sequence\n",
      "\n",
      "\n",
      "Vernacular name\n",
      "\n",
      "\n",
      "unigram\n",
      "bigram\n",
      "trigram\n",
      "\n",
      "\n",
      "Order of resulting Markov model\n",
      "\n",
      "\n",
      "0\n",
      "1\n",
      "2\n",
      "\n",
      "\n",
      "Protein sequencing\n",
      "amino acid\n",
      "… Cys-Gly-Leu-Ser-Trp …\n",
      "…, Cys, Gly, Leu, Ser, Trp, …\n",
      "…, Cys-Gly, Gly-Leu, Leu-Ser, Ser-Trp, …\n",
      "…, Cys-Gly-Leu, Gly-Leu-Ser, Leu-Ser-Trp, …\n",
      "\n",
      "\n",
      "DNA sequencing\n",
      "base pair\n",
      "…AGCTTCGA…\n",
      "…, A, G, C, T, T, C, G, A, …\n",
      "…, AG, GC, CT, TT, TC, CG, GA, …\n",
      "…, AGC, GCT, CTT, TTC, TCG, CGA, …\n",
      "\n",
      "\n",
      "Computational linguistics\n",
      "character\n",
      "…to_be_or_not_to_be…\n",
      "…, t, o, _, b, e, _, o, r, _, n, o, t, _, t, o, _, b, e, …\n",
      "…, to, o_, _b, be, e_, _o, or, r_, _n, no, ot, t_, _t, to, o_, _b, be, …\n",
      "…, to_, o_b, _be, be_, e_o, _or, or_, r_n, _no, not, ot_, t_t, _to, to_, o_b, _be, …\n",
      "\n",
      "\n",
      "Computational linguistics\n",
      "word\n",
      "… to be or not to be …\n",
      "…, to, be, or, not, to, be, …\n",
      "…, to be, be or, or not, not to, to be, …\n",
      "…, to be or, be or not, or not to, not to be, …\n",
      "\n",
      "\n",
      "Figure 1 shows several example sequences and the corresponding 1-gram, 2-gram and 3-gram sequences.\n",
      "Here are further examples; these are word-level 3-grams and 4-grams (and counts of the number of times they appeared) from the Google n-gram corpus.[3]\n",
      "\n",
      "ceramics collectables collectibles (55)\n",
      "ceramics collectables fine (130)\n",
      "ceramics collected by (52)\n",
      "ceramics collectible pottery (50)\n",
      "ceramics collectibles cooking (45)\n",
      "\n",
      "4-grams\n",
      "\n",
      "serve as the incoming (92)\n",
      "serve as the incubator (99)\n",
      "serve as the independent (794)\n",
      "serve as the index (223)\n",
      "serve as the indication (72)\n",
      "serve as the indicator (120)\n",
      "\n",
      "n-gram models[edit]\n",
      "An n-gram model models sequences, notably natural languages, using the statistical properties of n-grams.\n",
      "This idea can be traced to an experiment by Claude Shannon's work in information theory. Shannon posed the question: given a sequence of letters (for example, the sequence \"for ex\"), what is the likelihood of the next letter? From training data, one can derive a probability distribution for the next letter given a history of size : a = 0.4, b = 0.00001, c = 0, ....; where the probabilities of all possible \"next-letters\" sum to 1.0...\n",
      "More concisely, an n-gram model predicts  based on . In probability terms, this is . When used for language modeling, independence assumptions are made so that each word depends only on the last n − 1 words. This Markov model is used as an approximation of the true underlying language. This assumption is important because it massively simplifies the problem of learning the language model from data. In addition, because of the open nature of language, it is common to group words unknown to the language model together.\n",
      "Note that in a simple n-gram language model, the probability of a word, conditioned on some number of previous words (one word in a bigram model, two words in a trigram model, etc.) can be described as following a categorical distribution (often imprecisely called a \"multinomial distribution\").\n",
      "In practice, the probability distributions are smoothed by assigning non-zero probabilities to unseen words or n-grams; see smoothing techniques.\n",
      "Applications and considerations[edit]\n",
      "n-gram models are widely used in statistical natural language processing. In speech recognition, phonemes and sequences of phonemes are modeled using a n-gram distribution. For parsing, words are modeled such that each n-gram is composed of n words. For language identification, sequences of characters/graphemes (e.g., letters of the alphabet) are modeled for different languages.[4] For sequences of characters, the 3-grams (sometimes referred to as \"trigrams\") that can be generated from \"good morning\" are \"goo\", \"ood\", \"od \", \"d m\", \" mo\", \"mor\" and so forth (sometimes the beginning and end of a text are modeled explicitly, adding \"__g\", \"_go\", \"ng_\", and \"g__\"). For sequences of words, the trigrams that can be generated from \"the dog smelled like a skunk\" are \"# the dog\", \"the dog smelled\", \"dog smelled like\", \"smelled like a\", \"like a skunk\" and \"a skunk #\". Some[who?] practitioners preprocess strings to remove spaces, most[who?] simply collapse whitespace to a single space while preserving paragraph marks. Punctuation is also commonly reduced or removed by preprocessing. n-grams can also be used for sequences of words or almost any type of data. For example, they have been used for extracting features for clustering large sets of satellite earth images and for determining what part of the Earth a particular image came from.[5] They have also been very successful as the first pass in genetic sequence search and in the identification of the species from which short sequences of DNA originated.[6]\n",
      "n-gram models are often criticized because they lack any explicit representation of long range dependency. (In fact, it was Chomsky's critique of Markov models in the late 1950s that caused their virtual disappearance from natural language processing, along with statistical methods in general, until well into the 1980s.) This is because the only explicit dependency range is (n − 1) tokens for an n-gram model, and since natural languages incorporate many cases of unbounded dependencies (such as wh-movement), this means that an n-gram model cannot in principle distinguish unbounded dependencies from noise (since long range correlations drop exponentially with distance for any Markov model). For this reason, n-gram models have not made much impact on linguistic theory, where part of the explicit goal is to model such dependencies.\n",
      "Another criticism that has been made is that Markov models of language, including n-gram models, do not explicitly capture the performance/competence distinction discussed by Chomsky. This is because n-gram models are not designed to model linguistic knowledge as such, and make no claims to being (even potentially) complete models of linguistic knowledge; instead, they are used in practical applications.\n",
      "In practice, n-gram models have been shown to be extremely effective in modeling language data, which is a core component in modern statistical language applications. Most modern applications that rely on n-gram based models, such as machine translation applications, do not rely exclusively on such models; instead, they typically also incorporate Bayesian inference. Modern statistical models are typically made up of two parts, a prior distribution describing the inherent likelihood of a possible result and a likelihood function used to assess the compatibility of a possible result with observed data. When a language model is used, it is used as part of the prior distribution (e.g. to gauge the inherent \"goodness\" of a possible translation), and even then it is often not the only component in this distribution. Handcrafted features of various sorts are also used, for example variables that represent the position of a word in a sentence or the general topic of discourse. In addition, features based on the structure of the potential result, such as syntactic considerations, are often used. Such features are also used as part of the likelihood function, which makes use of the observed data. Conventional linguistic theory can be incorporated in these features (although in practice, it is rare that features specific to generative or other particular theories of grammar are incorporated, as computational linguists tend to be \"agnostic\" towards individual theories of grammar[citation needed]).\n",
      "n-grams for approximate matching[edit]\n",
      "Main article: Approximate string matching\n",
      "n-grams can also be used for efficient approximate matching. By converting a sequence of items to a set of n-grams, it can be embedded in a vector space, thus allowing the sequence to be compared to other sequences in an efficient manner. For example, if we convert strings with only letters in the English alphabet into single character 3-grams, we get a -dimensional space (the first dimension measures the number of occurrences of \"aaa\", the second \"aab\", and so forth for all possible combinations of three letters). Using this representation, we lose information about the string. For example, both the strings \"abc\" and \"bca\" give rise to exactly the same 2-gram \"bc\" (although {\"ab\", \"bc\"} is clearly not the same as {\"bc\", \"ca\"}). However, we know empirically that if two strings of real text have a similar vector representation (as measured by cosine distance) then they are likely to be similar. Other metrics have also been applied to vectors of n-grams with varying, sometimes better, results. For example z-scores have been used to compare documents by examining how many standard deviations each n-gram differs from its mean occurrence in a large collection, or text corpus, of documents (which form the \"background\" vector). In the event of small counts, the g-score may give better results for comparing alternative models.\n",
      "It is also possible to take a more principled approach to the statistics of n-grams, modeling similarity as the likelihood that two strings came from the same source directly in terms of a problem in Bayesian inference.\n",
      "n-gram-based searching can also be used for plagiarism detection.\n",
      "Other applications[edit]\n",
      "n-grams find use in several areas of computer science, computational linguistics, and applied mathematics.\n",
      "They have been used to:\n",
      "\n",
      "design kernels that allow machine learning algorithms such as support vector machines to learn from string data\n",
      "find likely candidates for the correct spelling of a misspelled word\n",
      "improve compression in compression algorithms where a small area of data requires n-grams of greater length\n",
      "assess the probability of a given word sequence appearing in text of a language of interest in pattern recognition systems, speech recognition, OCR (optical character recognition), Intelligent Character Recognition (ICR), machine translation and similar applications\n",
      "improve retrieval in information retrieval systems when it is hoped to find similar \"documents\" (a term for which the conventional meaning is sometimes stretched, depending on the data set) given a single query document and a database of reference documents\n",
      "improve retrieval performance in genetic sequence analysis as in the BLAST family of programs\n",
      "identify the language a text is in or the species a small sequence of DNA was taken from\n",
      "predict letters or words at random in order to create text, as in the dissociated press algorithm.\n",
      "\n",
      "Bias-versus-variance trade-off[edit]\n",
      "What goes into picking the n for the n-gram?\n",
      "With n-gram models it is necessary to find the right trade off between the stability of the estimate against its appropriateness. This means that trigram (i.e. triplets of words) is a common choice with large training corpora (millions of words), whereas a bigram is often used with smaller ones.\n",
      "Smoothing techniques[edit]\n",
      "There are problems of balance weight between infrequent grams (for example, if a proper name appeared in the training data) and frequent grams. Also, items not seen in the training data will be given a probability of 0.0 without smoothing. For unseen but plausible data from a sample, one can introduce pseudocounts. Pseudocounts are generally motivated on Bayesian grounds.\n",
      "In practice it is necessary to smooth the probability distributions by also assigning non-zero probabilities to unseen words or n-grams. The reason is that models derived directly from the n-gram frequency counts have severe problems when confronted with any n-grams that have not explicitly been seen before -- the zero-frequency problem. Various smoothing methods are used, from simple \"add-one\" (Laplace) smoothing (assign a count of 1 to unseen n-grams; see Rule of succession) to more sophisticated models, such as Good–Turing discounting or back-off models. Some of these methods are equivalent to assigning a prior distribution to the probabilities of the n-grams and using Bayesian inference to compute the resulting posterior n-gram probabilities. However, the more sophisticated smoothing models were typically not derived in this fashion, but instead through independent considerations.\n",
      "\n",
      "Linear interpolation (e.g., taking the weighted mean of the unigram, bigram, and trigram)\n",
      "Good–Turing discounting\n",
      "Witten–Bell discounting\n",
      "Lidstone's smoothing\n",
      "Katz's back-off model (trigram)\n",
      "Kneser–Ney smoothing\n",
      "\n",
      "Skip-gram[edit]\n",
      "In the field of computational linguistics, in particular language modeling, skip-grams[7] are a generalization of n-grams in which the components (typically words) need not be consecutive in the text under consideration, but may leave gaps that are skipped over.[8] They provide one way of overcoming the data sparsity problem found with conventional n-gram analysis.\n",
      "Formally, an n-gram is a consecutive subsequence of length n of some sequence of tokens w₁ … wₙ. A k-skip-n-gram is a length-n subsequence where the components occur at distance at most k from each other.\n",
      "For example, in the input text:\n",
      "\n",
      "the rain in Spain falls mainly on the plain\n",
      "\n",
      "the set of 1-skip-2-grams includes all the bigrams (2-grams), and in addition the subsequences\n",
      "\n",
      "the in, rain Spain, in falls, Spain mainly, falls on, mainly the, and on plain.\n",
      "\n",
      "Recently, Mikolov et al. (2013) have demonstrated that skip-gram language models can be trained so that it is possible to do ″word arithmetic\". In their model, for example the n-gram vector expression\n",
      "\n",
      "vector(\"king\") − vector(\"man\") + vector(\"woman\")\n",
      "\n",
      "evaluates very close to vector(\"queen\").[9]\n",
      "Syntactic n-grams[edit]\n",
      "Syntactic n-grams are n-grams defined by paths in syntactic dependency or constituent trees rather than the linear structure of the text.[10][11][12] For example, the sentence \"economic news has little effect on financial markets\" can be transformed to syntactic n-grams following the tree structure of its dependency relations: news-economic, effect-little, effect-on-markets-financial.[10]\n",
      "Syntactic n-grams are intended to reflect syntactic structure more faithfully than linear n-grams, and have many of the same applications, especially as features in a Vector Space Model. Syntactic n-grams for certain tasks gives better results than the use of standard n-grams, for example, for authorship attribution.[13]\n",
      "See also[edit]\n",
      "\n",
      "Collocation\n",
      "Hidden Markov model\n",
      "n-tuple\n",
      "k-mer\n",
      "String kernel\n",
      "MinHash\n",
      "Feature extraction\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "v\n",
      "t\n",
      "e\n",
      "\n",
      "\n",
      "Natural Language Processing\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "General Terms\n",
      "\n",
      "\n",
      "\n",
      "Text corpus\n",
      "Speech corpus\n",
      "Stopwords\n",
      "Bag-of-words\n",
      "AI-complete\n",
      "n-gram (Bigram, Trigrams)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text analysis\n",
      "\n",
      "\n",
      "\n",
      "Text segmentation\n",
      "POS Tagging\n",
      "Text Chunking\n",
      "Compound term processing\n",
      "Collocation extraction\n",
      "Stemming\n",
      "Lemmatisation\n",
      "NER\n",
      "Coreference resolution\n",
      "Sentiment analysis\n",
      "Concept mining\n",
      "Parsing\n",
      "Word sense disambiguation\n",
      "Terminology extraction\n",
      "Truecasing\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Automatic summarization\n",
      "\n",
      "\n",
      "\n",
      "Multi-document summarization\n",
      "Sentence extraction\n",
      "Text simplification\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Machine Translation\n",
      "\n",
      "\n",
      "\n",
      "Computer-assisted\n",
      "Example Based\n",
      "Rule Based\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "AIDC\n",
      "\n",
      "\n",
      "\n",
      "Speech recognition\n",
      "Speech synthesis\n",
      "OCR\n",
      "Natural language generation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Topic model\n",
      "\n",
      "\n",
      "\n",
      "Pachinko allocation\n",
      "LDA\n",
      "LSA\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Computer-assisted reviewing\n",
      "\n",
      "\n",
      "\n",
      "Grammar checker\n",
      "Automated essay scoring\n",
      "Concordancer\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Natural language user interface\n",
      "\n",
      "\n",
      "\n",
      "Question answering\n",
      "Chatterbot\n",
      "Automated online assistant\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "References[edit]\n",
      "\n",
      "\n",
      "^ Broder, Andrei Z.; Glassman, Steven C.; Manasse, Mark S.; Zweig, Geoffrey (1997). \"Syntactic clustering of the web\". Computer Networks and ISDN Systems 29 (8): 1157–1166. \n",
      "^ https://class.coursera.org/nlp/lecture/17\n",
      "^ Alex Franz and Thorsten Brants (2006). \"All Our N-gram are Belong to You\". Google Research Blog. Retrieved 2011-12-16. \n",
      "^ Ted Dunning (1994). \"Statistical Identification of Language\". New Mexico State University.  Technical Report MCCS 94-273\n",
      "^ Soffer, A (1997). \"Image categorization using texture features\". Proceedings of the Fourth International Conference on 1 (233): 237. doi:10.1109/ICDAR.1997.619847. \n",
      "^ Tomović, Andrija; Janičić, Predrag; Kešelj, Vlado (2006). \"n-Gram-based classification and unsupervised hierarchical clustering of genome sequences\". Computer Methods and Programs in Biomedicine 81 (2): 137–153. doi:10.1016/j.cmpb.2005.11.007. \n",
      "^ http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.45.1629\n",
      "^ David Guthrie et al. (2006). \"A Closer Look at Skip-gram Modelling\" (PDF).  CS1 maint: Explicit use of et al. (link)\n",
      "^ Tomáš Mikolov et al. (2013). \"Efficient Estimation of Word Representations in Vector Space\".  CS1 maint: Explicit use of et al. (link)\n",
      "^ a b Sidorov, Grigori; Velazquez, Francisco; Stamatatos, Efstathios; Gelbukh, Alexander; Chanona-Hernández, Liliana (2012). \"Syntactic Dependency-based n-grams as Classification Features\". LNAI 7630: 1–11. \n",
      "^ Sidorov, Grigori (2013). \"Syntactic Dependency-Based n-grams in Rule Based Automatic English as Second Language Grammar Correction\". International Journal of Computational Linguistics and Applications 4 (2): 169–188. \n",
      "^ Figueroa, Alejandro; Atkinson, John (2012). \"Contextual Language Models For Ranking Answers To Natural Language Definition Questions\". Computational Intelligence 28 (4): 528––548. \n",
      "^ Sidorov, Grigori; Velasquez, Francisco; Stamatatos, Efstathios; Gelbukh, Alexander; Chanona-Hernández, Liliana. \"Syntactic n-Grams as Machine Learning Features for Natural Language Processing\". Expert Systems with Applications 41 (3): 853–860. doi:10.1016/j.eswa.2013.08.015. \n",
      "\n",
      "\n",
      "\n",
      "Christopher D. Manning, Hinrich Schütze, Foundations of Statistical Natural Language Processing, MIT Press: 1999. ISBN 0-262-13360-1.\n",
      "White, Owen; Dunning, Ted; Sutton, Granger; Adams, Mark; Venter, J.Craig; Fields, Chris (1993). \"A quality control algorithm for dna sequencing projects\". Nucleic Acids Research 21 (16): 3829–3838. \n",
      "Frederick J. Damerau, Markov Models and Linguistic Theory. Mouton. The Hague, 1971.\n",
      "Figueroa, Alejandro; Atkinson, John (2012). \"Contextual Language Models For Ranking Answers To Natural Language Definition Questions\". Computational Intelligence 28 (4): 528––548. \n",
      "Brocardo, Marcelo Luiz; Issa Traore; Sherif Saad; Isaac Woungang (2013). Authorship Verification for Short Messages Using Stylometry (PDF). IEEE Intl. Conference on Computer, Information and Telecommunication Systems (CITS). \n",
      "\n",
      "External links[edit]\n",
      "\n",
      "Google's Google Book n-gram viewer and Web n-grams database (September 2006)\n",
      "Microsoft's web n-grams service\n",
      "1,000,000 most frequent 2,3,4,5-grams from the 425 million word Corpus of Contemporary American English\n",
      "Peachnote's music ngram viewer\n",
      "Stochastic Language Models (n-Gram) Specification (W3C)\n",
      "Michael Collin's notes on n-Gram Language Models\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\t\t\t\t\t\tRetrieved from \"https://en.wikipedia.org/w/index.php?title=N-gram&oldid=669294742\"\t\t\t\t\t\n",
      "Categories: Natural language processingComputational linguisticsLanguage modelingSpeech recognitionCorpus linguisticsProbabilistic modelsHidden categories: CS1 maint: Explicit use of et al.Articles to be merged from May 2015All articles to be mergedArticles lacking in-text citations from February 2011All articles lacking in-text citationsAll articles with specifically marked weasel-worded phrasesArticles with specifically marked weasel-worded phrases from November 2011Articles with specifically marked weasel-worded phrases from June 2014All articles with unsourced statementsArticles with unsourced statements from November 2011 \n",
      "\n",
      "\n",
      "\n",
      "Navigation menu\n",
      "\n",
      "\n",
      "Personal tools\n",
      "\n",
      "Create accountLog in \n",
      "\n",
      "\n",
      "\n",
      "Namespaces\n",
      "\n",
      "Article\n",
      "Talk\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Variants\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "Read\n",
      "Edit\n",
      "View history\n",
      "\n",
      "\n",
      "\n",
      "More\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Navigation\n",
      "\n",
      "\n",
      "Main pageContentsFeatured contentCurrent eventsRandom articleDonate to WikipediaWikipedia store \n",
      "\n",
      "\n",
      "\n",
      "Interaction\n",
      "\n",
      "\n",
      "HelpAbout WikipediaCommunity portalRecent changesContact page \n",
      "\n",
      "\n",
      "\n",
      "Tools\n",
      "\n",
      "\n",
      "What links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationWikidata itemCite this page \n",
      "\n",
      "\n",
      "\n",
      "Print/export\n",
      "\n",
      "\n",
      "Create a bookDownload as PDFPrintable version \n",
      "\n",
      "\n",
      "\n",
      "Languages\n",
      "\n",
      "\n",
      "CatalàČeštinaDeutschEspañolEuskaraFrançaisItalianoNorsk bokmålОлык марийPolskiРусскийSlovenčinaSuomiУкраїнська \n",
      "Edit links \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " This page was last modified on 30 June 2015, at 06:20.\n",
      "Text is available under the Creative Commons Attribution-ShareAlike License;\n",
      "additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\n",
      "\n",
      "\n",
      "Privacy policy\n",
      "About Wikipedia\n",
      "Disclaimers\n",
      "Contact Wikipedia\n",
      "Developers\n",
      "Mobile view\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "if(window.jQuery)jQuery.ready();if(window.mw){\n",
      "mw.loader.state({\"ext.globalCssJs.site\":\"ready\",\"ext.globalCssJs.user\":\"ready\",\"site\":\"loading\",\"user\":\"ready\",\"user.groups\":\"ready\"});\n",
      "}\n",
      "\n",
      "if(window.mw){\n",
      "mw.loader.load([\"ext.cite.a11y\",\"mediawiki.toc\",\"mediawiki.action.view.postEdit\",\"mediawiki.user\",\"mediawiki.hidpi\",\"mediawiki.page.ready\",\"mediawiki.searchSuggest\",\"ext.cirrusSearch.loggingSchema\",\"mmv.bootstrap.autostart\",\"ext.imageMetrics.loader\",\"ext.visualEditor.targetLoader\",\"ext.eventLogging.subscriber\",\"ext.wikimediaEvents\",\"ext.wikimediaEvents.statsd\",\"ext.navigationTiming\",\"schema.UniversalLanguageSelector\",\"ext.uls.eventlogger\",\"ext.uls.interlanguage\",\"ext.gadget.teahouse\",\"ext.gadget.ReferenceTooltips\",\"ext.gadget.WatchlistGreenIndicators\",\"ext.gadget.DRN-wizard\",\"ext.gadget.charinsert\",\"ext.gadget.refToolbar\",\"ext.gadget.switcher\",\"ext.gadget.featured-articles-links\"],null,true);\n",
      "}\n",
      "if(window.mw){\n",
      "document.write(\"\\u003Cscript src=\\\"//en.wikipedia.org/w/load.php?debug=false\\u0026amp;lang=en\\u0026amp;modules=site\\u0026amp;only=scripts\\u0026amp;skin=vector\\u0026amp;*\\\"\\u003E\\u003C/script\\u003E\");\n",
      "}\n",
      "if(window.mw){\n",
      "mw.config.set({\"wgBackendResponseTime\":549,\"wgHostname\":\"mw1257\"});\n",
      "}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/N-gram\"\n",
    "page = urllib2.urlopen(url)\n",
    "soup = BeautifulSoup(page).get_text()\n",
    "print soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'n-gram - Wikipedia, the free encyclopedia a:lang(ar),a:lang(kk-arab),a:lang(mzn),a:lang(ps),a:lang(ur){text-decoration:none} /* cache key: global:resourceloader:filter:minify-css:7:de1ab5287c9076b96eedd3f97a84a7b6 */ if(window.mw){ mw.config.set({\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"N-gram\",\"wgTitle\":\"N-gram\",\"wgCurRevisionId\":669294742,\"wgRevisionId\":669294742,\"wgArticleId\":986182,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"CS1 maint: Explicit use of et al.\",\"Articles to be merged from May 2015\",\"All articles to be merged\",\"Articles lacking in-text citations from February 2011\",\"All articles lacking in-text citations\",\"All articles with specifically marked weasel-worded phrases\",\"Articles with specifically marked weasel-worded phrases from November 2011\",\"Articles with specifically marked weasel-worded phrases from June 2014\",\"All articles with unsourced statements\",\"Articles with unsourced statements from November 2011\",\"Natural language processing\",\"Computational linguistics\",\"Language modeling\",\"Speech recognition\",\"Corpus linguistics\",\"Probabilistic models\"],\"wgBreakFrames\":false,\"wgPageContentLanguage\":\"en\",\"wgPageContentModel\":\"wikitext\",\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgMonthNamesShort\":[\"\",\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"],\"wgRelevantPageName\":\"N-gram\",\"wgRelevantArticleId\":986182,\"wgIsProbablyEditable\":true,\"wgRestrictionEdit\":[],\"wgRestrictionMove\":[],\"wgMediaViewerOnClick\":true,\"wgMediaViewerEnabledByDefault\":true,\"wgVisualEditor\":{\"pageLanguageCode\":\"en\",\"pageLanguageDir\":\"ltr\",\"usePageImages\":true,\"usePageDescriptions\":true},\"wikilove-recipient\":\"\",\"wikilove-anon\":0,\"wgPoweredByHHVM\":true,\"wgULSAcceptLanguageList\":[\"en-us\",\"en\"],\"wgULSCurrentAutonym\":\"English\",\"wgWikiEditorEnabledModules\":{\"toolbar\":true,\"dialogs\":true,\"hidesig\":true,\"preview\":false,\"publish\":false},\"wgBetaFeaturesFeatures\":[],\"wgGatherShouldShowTutorial\":true,\"wgFlaggedRevsParams\":{\"tags\":{\"status\":{\"levels\":1,\"quality\":2,\"pristine\":3}}},\"wgStableRevisionId\":null,\"wgCategoryTreePageCategoryOptions\":\"{\\\\\"mode\\\\\":0,\\\\\"hideprefix\\\\\":20,\\\\\"showcount\\\\\":true,\\\\\"namespaces\\\\\":false}\",\"wgNoticeProject\":\"wikipedia\",\"wgWikibaseItemId\":\"Q94489\"}); }if(window.mw){ mw.loader.implement(\"user.options\",function($,jQuery){mw.user.options.set({\"variant\":\"en\"});}); /* cache key: global:resourceloader:filter:minify-js:7:b2706269305541eba923c165462b22c4 */ } if(window.mw){ mw.loader.implement(\"user.tokens\",function($,jQuery){mw.user.tokens.set({\"editToken\":\"+\\\\\\\\\",\"patrolToken\":\"+\\\\\\\\\",\"watchToken\":\"+\\\\\\\\\"});}); } if(window.mw){ mw.loader.load([\"mediawiki.page.startup\",\"mediawiki.legacy.wikibits\",\"mediawiki.legacy.ajax\",\"ext.centralauth.centralautologin\",\"mmv.head\",\"ext.imageMetrics.head\",\"ext.visualEditor.viewPageTarget.init\",\"ext.uls.init\",\"ext.uls.interface\",\"ext.centralNotice.bannerController\",\"skins.vector.js\"]); } n-gram From Wikipedia, the free encyclopedia Jump to: navigation, search It has been suggested that this article be merged with K-mer. (Discuss) Proposed since May 2015. Not to be confused with engram. For Google phrase-usage graphs, see Google Ngram Viewer. This article includes a list of references, but its sources remain unclear because it has insufficient inline citations. Please help to improve this article by introducing more precise citations. (February 2011) In the fields of computational linguistics and probability, an n-gram is a contiguous sequence of n items from a given sequence of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application. The n-grams typically are collected from a text or speech corpus. When the items are words, n-grams may also be called shingles.[1] An n-gram of size 1 is referred to as a \"unigram\"; size 2 is a \"bigram\" (or, less commonly, a \"digram\"); size 3 is a \"trigram\". Larger sizes are sometimes referred to by the value of n, e.g., \"four-gram\", \"five-gram\", and so on. Contents 1 Applications 2 Examples 3 n-gram models 4 Applications and considerations 5 n-grams for approximate matching 6 Other applications 7 Bias-versus-variance trade-off 7.1 Smoothing techniques 7.2 Skip-gram 8 Syntactic n-grams 9 See also 10 References 11 External links Applications[edit] An n-gram model is a type of probabilistic language model for predicting the next item in such a sequence in the form of a (n \\u2212 1)\\u2013order Markov model.[2] n-gram models are now widely used in probability, communication theory, computational linguistics (for instance, statistical natural language processing), computational biology (for instance, biological sequence analysis), and data compression. The two core advantages[compared to?] of n-gram models (and algorithms that use them) are relative simplicity and the ability to scale up \\u2013 by simply increasing n a model can be used to store more context with a well-understood space\\u2013time tradeoff, enabling small experiments to scale up very efficiently. Examples[edit] Figure 1 n-gram examples from various disciplines Field Unit Sample sequence 1-gram sequence 2-gram sequence 3-gram sequence Vernacular name unigram bigram trigram Order of resulting Markov model 0 1 2 Protein sequencing amino acid \\u2026 Cys-Gly-Leu-Ser-Trp \\u2026 \\u2026, Cys, Gly, Leu, Ser, Trp, \\u2026 \\u2026, Cys-Gly, Gly-Leu, Leu-Ser, Ser-Trp, \\u2026 \\u2026, Cys-Gly-Leu, Gly-Leu-Ser, Leu-Ser-Trp, \\u2026 DNA sequencing base pair \\u2026AGCTTCGA\\u2026 \\u2026, A, G, C, T, T, C, G, A, \\u2026 \\u2026, AG, GC, CT, TT, TC, CG, GA, \\u2026 \\u2026, AGC, GCT, CTT, TTC, TCG, CGA, \\u2026 Computational linguistics character \\u2026to_be_or_not_to_be\\u2026 \\u2026, t, o, _, b, e, _, o, r, _, n, o, t, _, t, o, _, b, e, \\u2026 \\u2026, to, o_, _b, be, e_, _o, or, r_, _n, no, ot, t_, _t, to, o_, _b, be, \\u2026 \\u2026, to_, o_b, _be, be_, e_o, _or, or_, r_n, _no, not, ot_, t_t, _to, to_, o_b, _be, \\u2026 Computational linguistics word \\u2026 to be or not to be \\u2026 \\u2026, to, be, or, not, to, be, \\u2026 \\u2026, to be, be or, or not, not to, to be, \\u2026 \\u2026, to be or, be or not, or not to, not to be, \\u2026 Figure 1 shows several example sequences and the corresponding 1-gram, 2-gram and 3-gram sequences. Here are further examples; these are word-level 3-grams and 4-grams (and counts of the number of times they appeared) from the Google n-gram corpus.[3] ceramics collectables collectibles (55) ceramics collectables fine (130) ceramics collected by (52) ceramics collectible pottery (50) ceramics collectibles cooking (45) 4-grams serve as the incoming (92) serve as the incubator (99) serve as the independent (794) serve as the index (223) serve as the indication (72) serve as the indicator (120) n-gram models[edit] An n-gram model models sequences, notably natural languages, using the statistical properties of n-grams. This idea can be traced to an experiment by Claude Shannon\\'s work in information theory. Shannon posed the question: given a sequence of letters (for example, the sequence \"for ex\"), what is the likelihood of the next letter? From training data, one can derive a probability distribution for the next letter given a history of size : a = 0.4, b = 0.00001, c = 0, ....; where the probabilities of all possible \"next-letters\" sum to 1.0... More concisely, an n-gram model predicts based on . In probability terms, this is . When used for language modeling, independence assumptions are made so that each word depends only on the last n \\u2212 1 words. This Markov model is used as an approximation of the true underlying language. This assumption is important because it massively simplifies the problem of learning the language model from data. In addition, because of the open nature of language, it is common to group words unknown to the language model together. Note that in a simple n-gram language model, the probability of a word, conditioned on some number of previous words (one word in a bigram model, two words in a trigram model, etc.) can be described as following a categorical distribution (often imprecisely called a \"multinomial distribution\"). In practice, the probability distributions are smoothed by assigning non-zero probabilities to unseen words or n-grams; see smoothing techniques. Applications and considerations[edit] n-gram models are widely used in statistical natural language processing. In speech recognition, phonemes and sequences of phonemes are modeled using a n-gram distribution. For parsing, words are modeled such that each n-gram is composed of n words. For language identification, sequences of characters/graphemes (e.g., letters of the alphabet) are modeled for different languages.[4] For sequences of characters, the 3-grams (sometimes referred to as \"trigrams\") that can be generated from \"good morning\" are \"goo\", \"ood\", \"od \", \"d m\", \" mo\", \"mor\" and so forth (sometimes the beginning and end of a text are modeled explicitly, adding \"__g\", \"_go\", \"ng_\", and \"g__\"). For sequences of words, the trigrams that can be generated from \"the dog smelled like a skunk\" are \"# the dog\", \"the dog smelled\", \"dog smelled like\", \"smelled like a\", \"like a skunk\" and \"a skunk #\". Some[who?] practitioners preprocess strings to remove spaces, most[who?] simply collapse whitespace to a single space while preserving paragraph marks. Punctuation is also commonly reduced or removed by preprocessing. n-grams can also be used for sequences of words or almost any type of data. For example, they have been used for extracting features for clustering large sets of satellite earth images and for determining what part of the Earth a particular image came from.[5] They have also been very successful as the first pass in genetic sequence search and in the identification of the species from which short sequences of DNA originated.[6] n-gram models are often criticized because they lack any explicit representation of long range dependency. (In fact, it was Chomsky\\'s critique of Markov models in the late 1950s that caused their virtual disappearance from natural language processing, along with statistical methods in general, until well into the 1980s.) This is because the only explicit dependency range is (n \\u2212 1) tokens for an n-gram model, and since natural languages incorporate many cases of unbounded dependencies (such as wh-movement), this means that an n-gram model cannot in principle distinguish unbounded dependencies from noise (since long range correlations drop exponentially with distance for any Markov model). For this reason, n-gram models have not made much impact on linguistic theory, where part of the explicit goal is to model such dependencies. Another criticism that has been made is that Markov models of language, including n-gram models, do not explicitly capture the performance/competence distinction discussed by Chomsky. This is because n-gram models are not designed to model linguistic knowledge as such, and make no claims to being (even potentially) complete models of linguistic knowledge; instead, they are used in practical applications. In practice, n-gram models have been shown to be extremely effective in modeling language data, which is a core component in modern statistical language applications. Most modern applications that rely on n-gram based models, such as machine translation applications, do not rely exclusively on such models; instead, they typically also incorporate Bayesian inference. Modern statistical models are typically made up of two parts, a prior distribution describing the inherent likelihood of a possible result and a likelihood function used to assess the compatibility of a possible result with observed data. When a language model is used, it is used as part of the prior distribution (e.g. to gauge the inherent \"goodness\" of a possible translation), and even then it is often not the only component in this distribution. Handcrafted features of various sorts are also used, for example variables that represent the position of a word in a sentence or the general topic of discourse. In addition, features based on the structure of the potential result, such as syntactic considerations, are often used. Such features are also used as part of the likelihood function, which makes use of the observed data. Conventional linguistic theory can be incorporated in these features (although in practice, it is rare that features specific to generative or other particular theories of grammar are incorporated, as computational linguists tend to be \"agnostic\" towards individual theories of grammar[citation needed]). n-grams for approximate matching[edit] Main article: Approximate string matching n-grams can also be used for efficient approximate matching. By converting a sequence of items to a set of n-grams, it can be embedded in a vector space, thus allowing the sequence to be compared to other sequences in an efficient manner. For example, if we convert strings with only letters in the English alphabet into single character 3-grams, we get a -dimensional space (the first dimension measures the number of occurrences of \"aaa\", the second \"aab\", and so forth for all possible combinations of three letters). Using this representation, we lose information about the string. For example, both the strings \"abc\" and \"bca\" give rise to exactly the same 2-gram \"bc\" (although {\"ab\", \"bc\"} is clearly not the same as {\"bc\", \"ca\"}). However, we know empirically that if two strings of real text have a similar vector representation (as measured by cosine distance) then they are likely to be similar. Other metrics have also been applied to vectors of n-grams with varying, sometimes better, results. For example z-scores have been used to compare documents by examining how many standard deviations each n-gram differs from its mean occurrence in a large collection, or text corpus, of documents (which form the \"background\" vector). In the event of small counts, the g-score may give better results for comparing alternative models. It is also possible to take a more principled approach to the statistics of n-grams, modeling similarity as the likelihood that two strings came from the same source directly in terms of a problem in Bayesian inference. n-gram-based searching can also be used for plagiarism detection. Other applications[edit] n-grams find use in several areas of computer science, computational linguistics, and applied mathematics. They have been used to: design kernels that allow machine learning algorithms such as support vector machines to learn from string data find likely candidates for the correct spelling of a misspelled word improve compression in compression algorithms where a small area of data requires n-grams of greater length assess the probability of a given word sequence appearing in text of a language of interest in pattern recognition systems, speech recognition, OCR (optical character recognition), Intelligent Character Recognition (ICR), machine translation and similar applications improve retrieval in information retrieval systems when it is hoped to find similar \"documents\" (a term for which the conventional meaning is sometimes stretched, depending on the data set) given a single query document and a database of reference documents improve retrieval performance in genetic sequence analysis as in the BLAST family of programs identify the language a text is in or the species a small sequence of DNA was taken from predict letters or words at random in order to create text, as in the dissociated press algorithm. Bias-versus-variance trade-off[edit] What goes into picking the n for the n-gram? With n-gram models it is necessary to find the right trade off between the stability of the estimate against its appropriateness. This means that trigram (i.e. triplets of words) is a common choice with large training corpora (millions of words), whereas a bigram is often used with smaller ones. Smoothing techniques[edit] There are problems of balance weight between infrequent grams (for example, if a proper name appeared in the training data) and frequent grams. Also, items not seen in the training data will be given a probability of 0.0 without smoothing. For unseen but plausible data from a sample, one can introduce pseudocounts. Pseudocounts are generally motivated on Bayesian grounds. In practice it is necessary to smooth the probability distributions by also assigning non-zero probabilities to unseen words or n-grams. The reason is that models derived directly from the n-gram frequency counts have severe problems when confronted with any n-grams that have not explicitly been seen before -- the zero-frequency problem. Various smoothing methods are used, from simple \"add-one\" (Laplace) smoothing (assign a count of 1 to unseen n-grams; see Rule of succession) to more sophisticated models, such as Good\\u2013Turing discounting or back-off models. Some of these methods are equivalent to assigning a prior distribution to the probabilities of the n-grams and using Bayesian inference to compute the resulting posterior n-gram probabilities. However, the more sophisticated smoothing models were typically not derived in this fashion, but instead through independent considerations. Linear interpolation (e.g., taking the weighted mean of the unigram, bigram, and trigram) Good\\u2013Turing discounting Witten\\u2013Bell discounting Lidstone\\'s smoothing Katz\\'s back-off model (trigram) Kneser\\u2013Ney smoothing Skip-gram[edit] In the field of computational linguistics, in particular language modeling, skip-grams[7] are a generalization of n-grams in which the components (typically words) need not be consecutive in the text under consideration, but may leave gaps that are skipped over.[8] They provide one way of overcoming the data sparsity problem found with conventional n-gram analysis. Formally, an n-gram is a consecutive subsequence of length n of some sequence of tokens w\\u2081 \\u2026 w\\u2099. A k-skip-n-gram is a length-n subsequence where the components occur at distance at most k from each other. For example, in the input text: the rain in Spain falls mainly on the plain the set of 1-skip-2-grams includes all the bigrams (2-grams), and in addition the subsequences the in, rain Spain, in falls, Spain mainly, falls on, mainly the, and on plain. Recently, Mikolov et al. (2013) have demonstrated that skip-gram language models can be trained so that it is possible to do \\u2033word arithmetic\". In their model, for example the n-gram vector expression vector(\"king\") \\u2212 vector(\"man\") + vector(\"woman\") evaluates very close to vector(\"queen\").[9] Syntactic n-grams[edit] Syntactic n-grams are n-grams defined by paths in syntactic dependency or constituent trees rather than the linear structure of the text.[10][11][12] For example, the sentence \"economic news has little effect on financial markets\" can be transformed to syntactic n-grams following the tree structure of its dependency relations: news-economic, effect-little, effect-on-markets-financial.[10] Syntactic n-grams are intended to reflect syntactic structure more faithfully than linear n-grams, and have many of the same applications, especially as features in a Vector Space Model. Syntactic n-grams for certain tasks gives better results than the use of standard n-grams, for example, for authorship attribution.[13] See also[edit] Collocation Hidden Markov model n-tuple k-mer String kernel MinHash Feature extraction v t e Natural Language Processing General Terms Text corpus Speech corpus Stopwords Bag-of-words AI-complete n-gram (Bigram, Trigrams) Text analysis Text segmentation POS Tagging Text Chunking Compound term processing Collocation extraction Stemming Lemmatisation NER Coreference resolution Sentiment analysis Concept mining Parsing Word sense disambiguation Terminology extraction Truecasing Automatic summarization Multi-document summarization Sentence extraction Text simplification Machine Translation Computer-assisted Example Based Rule Based AIDC Speech recognition Speech synthesis OCR Natural language generation Topic model Pachinko allocation LDA LSA Computer-assisted reviewing Grammar checker Automated essay scoring Concordancer Natural language user interface Question answering Chatterbot Automated online assistant References[edit] ^ Broder, Andrei Z.; Glassman, Steven C.; Manasse, Mark S.; Zweig, Geoffrey (1997). \"Syntactic clustering of the web\". Computer Networks and ISDN Systems 29 (8): 1157\\u20131166. ^ https://class.coursera.org/nlp/lecture/17 ^ Alex Franz and Thorsten Brants (2006). \"All Our N-gram are Belong to You\". Google Research Blog. Retrieved 2011-12-16. ^ Ted Dunning (1994). \"Statistical Identification of Language\". New Mexico State University. Technical Report MCCS 94-273 ^ Soffer, A (1997). \"Image categorization using texture features\". Proceedings of the Fourth International Conference on 1 (233): 237. doi:10.1109/ICDAR.1997.619847. ^ Tomovi\\u0107, Andrija; Jani\\u010di\\u0107, Predrag; Ke\\u0161elj, Vlado (2006). \"n-Gram-based classification and unsupervised hierarchical clustering of genome sequences\". Computer Methods and Programs in Biomedicine 81 (2): 137\\u2013153. doi:10.1016/j.cmpb.2005.11.007. ^ http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.45.1629 ^ David Guthrie et al. (2006). \"A Closer Look at Skip-gram Modelling\" (PDF). CS1 maint: Explicit use of et al. (link) ^ Tom\\xe1\\u0161 Mikolov et al. (2013). \"Efficient Estimation of Word Representations in Vector Space\". CS1 maint: Explicit use of et al. (link) ^ a b Sidorov, Grigori; Velazquez, Francisco; Stamatatos, Efstathios; Gelbukh, Alexander; Chanona-Hern\\xe1ndez, Liliana (2012). \"Syntactic Dependency-based n-grams as Classification Features\". LNAI 7630: 1\\u201311. ^ Sidorov, Grigori (2013). \"Syntactic Dependency-Based n-grams in Rule Based Automatic English as Second Language Grammar Correction\". International Journal of Computational Linguistics and Applications 4 (2): 169\\u2013188. ^ Figueroa, Alejandro; Atkinson, John (2012). \"Contextual Language Models For Ranking Answers To Natural Language Definition Questions\". Computational Intelligence 28 (4): 528\\u2013\\u2013548. ^ Sidorov, Grigori; Velasquez, Francisco; Stamatatos, Efstathios; Gelbukh, Alexander; Chanona-Hern\\xe1ndez, Liliana. \"Syntactic n-Grams as Machine Learning Features for Natural Language Processing\". Expert Systems with Applications 41 (3): 853\\u2013860. doi:10.1016/j.eswa.2013.08.015. Christopher D. Manning, Hinrich Sch\\xfctze, Foundations of Statistical Natural Language Processing, MIT Press: 1999. ISBN 0-262-13360-1. White, Owen; Dunning, Ted; Sutton, Granger; Adams, Mark; Venter, J.Craig; Fields, Chris (1993). \"A quality control algorithm for dna sequencing projects\". Nucleic Acids Research 21 (16): 3829\\u20133838. Frederick J. Damerau, Markov Models and Linguistic Theory. Mouton. The Hague, 1971. Figueroa, Alejandro; Atkinson, John (2012). \"Contextual Language Models For Ranking Answers To Natural Language Definition Questions\". Computational Intelligence 28 (4): 528\\u2013\\u2013548. Brocardo, Marcelo Luiz; Issa Traore; Sherif Saad; Isaac Woungang (2013). Authorship Verification for Short Messages Using Stylometry (PDF). IEEE Intl. Conference on Computer, Information and Telecommunication Systems (CITS). External links[edit] Google\\'s Google Book n-gram viewer and Web n-grams database (September 2006) Microsoft\\'s web n-grams service 1,000,000 most frequent 2,3,4,5-grams from the 425 million word Corpus of Contemporary American English Peachnote\\'s music ngram viewer Stochastic Language Models (n-Gram) Specification (W3C) Michael Collin\\'s notes on n-Gram Language Models Retrieved from \"https://en.wikipedia.org/w/index.php?title=N-gram&oldid=669294742\" Categories: Natural language processingComputational linguisticsLanguage modelingSpeech recognitionCorpus linguisticsProbabilistic modelsHidden categories: CS1 maint: Explicit use of et al.Articles to be merged from May 2015All articles to be mergedArticles lacking in-text citations from February 2011All articles lacking in-text citationsAll articles with specifically marked weasel-worded phrasesArticles with specifically marked weasel-worded phrases from November 2011Articles with specifically marked weasel-worded phrases from June 2014All articles with unsourced statementsArticles with unsourced statements from November 2011 Navigation menu Personal tools Create accountLog in Namespaces Article Talk Variants Views Read Edit View history More Search Navigation Main pageContentsFeatured contentCurrent eventsRandom articleDonate to WikipediaWikipedia store Interaction HelpAbout WikipediaCommunity portalRecent changesContact page Tools What links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationWikidata itemCite this page Print/export Create a bookDownload as PDFPrintable version Languages Catal\\xe0\\u010ce\\u0161tinaDeutschEspa\\xf1olEuskaraFran\\xe7aisItalianoNorsk bokm\\xe5l\\u041e\\u043b\\u044b\\u043a \\u043c\\u0430\\u0440\\u0438\\u0439Polski\\u0420\\u0443\\u0441\\u0441\\u043a\\u0438\\u0439Sloven\\u010dinaSuomi\\u0423\\u043a\\u0440\\u0430\\u0457\\u043d\\u0441\\u044c\\u043a\\u0430 Edit links This page was last modified on 30 June 2015, at 06:20. Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia\\xae is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Developers Mobile view if(window.jQuery)jQuery.ready();if(window.mw){ mw.loader.state({\"ext.globalCssJs.site\":\"ready\",\"ext.globalCssJs.user\":\"ready\",\"site\":\"loading\",\"user\":\"ready\",\"user.groups\":\"ready\"}); } if(window.mw){ mw.loader.load([\"ext.cite.a11y\",\"mediawiki.toc\",\"mediawiki.action.view.postEdit\",\"mediawiki.user\",\"mediawiki.hidpi\",\"mediawiki.page.ready\",\"mediawiki.searchSuggest\",\"ext.cirrusSearch.loggingSchema\",\"mmv.bootstrap.autostart\",\"ext.imageMetrics.loader\",\"ext.visualEditor.targetLoader\",\"ext.eventLogging.subscriber\",\"ext.wikimediaEvents\",\"ext.wikimediaEvents.statsd\",\"ext.navigationTiming\",\"schema.UniversalLanguageSelector\",\"ext.uls.eventlogger\",\"ext.uls.interlanguage\",\"ext.gadget.teahouse\",\"ext.gadget.ReferenceTooltips\",\"ext.gadget.WatchlistGreenIndicators\",\"ext.gadget.DRN-wizard\",\"ext.gadget.charinsert\",\"ext.gadget.refToolbar\",\"ext.gadget.switcher\",\"ext.gadget.featured-articles-links\"],null,true); } if(window.mw){ document.write(\"\\\\u003Cscript src=\\\\\"//en.wikipedia.org/w/load.php?debug=false\\\\u0026amp;lang=en\\\\u0026amp;modules=site\\\\u0026amp;only=scripts\\\\u0026amp;skin=vector\\\\u0026amp;*\\\\\"\\\\u003E\\\\u003C/script\\\\u003E\"); } if(window.mw){ mw.config.set({\"wgBackendResponseTime\":549,\"wgHostname\":\"mw1257\"}); }'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined = ' '.join(soup.split())\n",
    "joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exclude = set(string.punctuation)\n",
    "without_punctuation = ''.join(ch for ch in joined if ch not in exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'2011All', u'articles', u'lacking'),\n",
       " (u'2011Articles', u'with', u'specifically'),\n",
       " (u'CS1', u'maint', u'Explicit'),\n",
       " (u'Explicit', u'use', u'of'),\n",
       " (u'Natural', u'Language', u'Processing'),\n",
       " (u'also', u'be', u'used'),\n",
       " (u'an', u'ngram', u'model'),\n",
       " (u'be', u'or', u'not'),\n",
       " (u'be', u'used', u'for'),\n",
       " (u'be', u'\\u2026', u'\\u2026'),\n",
       " (u'can', u'also', u'be'),\n",
       " (u'have', u'been', u'used'),\n",
       " (u'maint', u'Explicit', u'use'),\n",
       " (u'marked', u'weaselworded', u'phrases'),\n",
       " (u'n', u'\\u2212', u'1'),\n",
       " (u'natural', u'language', u'processing'),\n",
       " (u'ngram', u'models', u'are'),\n",
       " (u'not', u'to', u'be'),\n",
       " (u'of', u'a', u'possible'),\n",
       " (u'or', u'not', u'to'),\n",
       " (u'part', u'of', u'the'),\n",
       " (u'serve', u'as', u'the'),\n",
       " (u'specifically', u'marked', u'weaselworded'),\n",
       " (u'to', u'be', u'or'),\n",
       " (u'to', u'be', u'\\u2026'),\n",
       " (u'use', u'of', u'et'),\n",
       " (u'weaselworded', u'phrases', u'from'),\n",
       " (u'with', u'specifically', u'marked'),\n",
       " (u'\\u2026', u'to', u'be'),\n",
       " (u'\\u2026', u'\\u2026', u'to')]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.wordpunct_tokenize(without_punctuation)\n",
    "finder = TrigramCollocationFinder.from_words(tokens)\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "scored = finder.score_ngrams(trigram_measures.raw_freq)\n",
    "set(trigram for trigram, score in scored) == set(nltk.trigrams(tokens))\n",
    "sorted(finder.nbest(trigram_measures.raw_freq, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
